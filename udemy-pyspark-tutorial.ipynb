{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dhirajmishra57/udemy-pyspark-tutorial?scriptVersionId=134135701\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Pyspark End-To-End\n\n`Course - https://www.udemy.com/course/pyspark-end-to-end-developer-course-spark-with-python/learn/lecture/37180040#overview`\n\n## Topics -\n    - How to read flat file and tables using jbdc connection\n    - DataFrame API Selection\n    - DataFrame Where & Filter\n    - DataFrame API Sorting\n    - DataFrame API Set\n    - DataFrame API Join\n    - DataFrame API Windowing\n    - DataFrame API Aggregation\n    - DataFrame API GroupBy\n    - DataFrame APIs Null Functions\n\n## Read & Write Files\n    - CSV\n    - JSON\n    - Parquet\n    - XML, Avro, Binary","metadata":{"execution":{"iopub.status.busy":"2023-06-16T06:27:18.779246Z","iopub.execute_input":"2023-06-16T06:27:18.779625Z","iopub.status.idle":"2023-06-16T06:27:18.784244Z","shell.execute_reply.started":"2023-06-16T06:27:18.779597Z","shell.execute_reply":"2023-06-16T06:27:18.783167Z"}}},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2023-06-19T13:59:27.549798Z","iopub.execute_input":"2023-06-19T13:59:27.550149Z","iopub.status.idle":"2023-06-19T14:00:07.17533Z","shell.execute_reply.started":"2023-06-19T13:59:27.550114Z","shell.execute_reply":"2023-06-19T14:00:07.173574Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317146 sha256=0d3391f1902f4a42fdc3a0ec0bce484f9ed1a3bb09dac1031124a6976cd7a43c\n  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pyspark\nfrom pyspark.sql import SparkSession,functions\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import StructField,StructType,IntegerType,StringType\nfrom pyspark.sql import window\nfrom pyspark.sql.window import *\n\nspark = SparkSession.builder.appName(\"Pyspark-Tutorial\").getOrCreate()\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-19T14:00:07.1776Z","iopub.execute_input":"2023-06-19T14:00:07.177948Z","iopub.status.idle":"2023-06-19T14:00:11.933911Z","shell.execute_reply.started":"2023-06-19T14:00:07.177924Z","shell.execute_reply":"2023-06-19T14:00:11.932611Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/06/19 14:00:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/retail-db-tutorial/RetailDB SalesData/Customers/part-00000\n/kaggle/input/retail-db-tutorial/RetailDB SalesData/Orders/part-00000\n/kaggle/input/retail-db-tutorial/RetailDB SalesData/Products/part-00000\n/kaggle/input/retail-db-tutorial/RetailDB SalesData/Order_items/part-00000\n/kaggle/input/retail-db-tutorial/RetailDB SalesData/Categories/part-00000\n/kaggle/input/retail-db-tutorial/RetailDB SalesData/Departments/part-00000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## How to read file using various techniques","metadata":{}},{"cell_type":"code","source":"ord_file_path = \"/kaggle/input/retail-db-tutorial/RetailDB SalesData/Orders\"\nord = spark.read.load(ord_file_path,sep=\",\",format='csv',schema=('ordid int,ord_date timestamp,ord_cust_id int, ord_status string'))\nord.show(1)\n\nord = spark.read.csv(ord_file_path,sep=\",\",schema=('ordid int,ord_date timestamp,ord_cust_id int, ord_status string'))\nord.show(1)\n\nord = spark.read.format('csv').load(ord_file_path,sep=',',schema='ordid int,ord_date timestamp,ord_cust_id int, ord_status string')\nord.show(1)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:10:49.984789Z","iopub.execute_input":"2023-06-19T06:10:49.985157Z","iopub.status.idle":"2023-06-19T06:10:57.912009Z","shell.execute_reply.started":"2023-06-19T06:10:49.985122Z","shell.execute_reply":"2023-06-19T06:10:57.910637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## How to read table and jdbc connection","metadata":{}},{"cell_type":"code","source":"\"\"\"\nspark.read.jdbc(\"connection string / jdbc:mysql://localhost:3306/emp\",\"table\",\n                properties={\"user\":\"root\",\"password\":\"root\",\"driver\":\"com.mysql.cj.jdbc.Driver\"})\n\nspark.read.format(\"jdbc\") \\\n.option(\"url\",\"connection string\") \\\n.option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n.option(\"user\",\"root\")\n.option(\"password\":\"root\")\n.option(\"dbtable\",\"employee\")\n# .option(\"dbtable\",\"select empid, salary from employee where gender = 'M' \")\n# .option(\"query\",\" select empid, salary from employee where gender = 'M' \")\n.load()\n\n\"\"\"\nprint(\"Hello\")","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:10:57.913447Z","iopub.execute_input":"2023-06-19T06:10:57.913967Z","iopub.status.idle":"2023-06-19T06:10:57.927316Z","shell.execute_reply.started":"2023-06-19T06:10:57.913917Z","shell.execute_reply":"2023-06-19T06:10:57.926246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataFrame API Selection\n\n### Select\nSelect one or more columns\nApplying necessary functions on select\n\n### SelectExpr\nThis is a variant of select that accepts SQL expressions.\nIf we want to use any functions available in SQL but not in Spark Built-in functions, then we can use selectExpr\n            -- explode\n\n### withColumn(colName, col)\nApplied transformation to only selected columns.\nThe first argument is an alias name. If we give an alias name same as a column name, the transformations will apply\non the same column.\nOtherwise a new column will be formed. Avoid giving alias name same as column name.\nord.withColumn('order_month',substring(orderDF.order_date,1,10)).show()\n\n\n### withColumnRenamed(existingCol, newCol)\nRename Existing Column.\nord.withColumnRenamed('order_id','order_id1').show()\n\n### drop(*cols)\nDrop a column.\norder = ord.drop('order_id','order_date')\n\n### dropDuplicates(subset=None)\nDrop duplicate rows.\nOptionally can consider only subset of columns.\nemp.dropDuplicates().show()\nemp.dropDuplicates((\"name\",\"score1\",\"score2\")).show()","metadata":{}},{"cell_type":"code","source":"ord\ndata = [('Robert',35,40,40),('Robert',35,40,40),('Ram',31,33,29),('Ram',31,33,91)]\nemp = spark.createDataFrame(data=data,schema=['name','score1','score2','score3'])","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:10:57.930669Z","iopub.execute_input":"2023-06-19T06:10:57.931121Z","iopub.status.idle":"2023-06-19T06:10:58.134584Z","shell.execute_reply.started":"2023-06-19T06:10:57.931078Z","shell.execute_reply":"2023-06-19T06:10:58.13311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ord.select(\"ordid\",'ordid',ord.ordid).show(5)\n\nord.select(lower(\"ord_status\").alias(\"lower_sts\"),\"ord_status\").show(5)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:10:58.135996Z","iopub.execute_input":"2023-06-19T06:10:58.136525Z","iopub.status.idle":"2023-06-19T06:10:58.833583Z","shell.execute_reply.started":"2023-06-19T06:10:58.136477Z","shell.execute_reply":"2023-06-19T06:10:58.832138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ord.show(5)\nord.selectExpr(\"substring(ord_date,1,4) as ord_year\",\"ord_date\") \\\n.selectExpr(\"ord_year\",\"substring(ord_date,6,2) as ord_month\",\"ord_date\") \\\n.selectExpr(\"ord_date\",\"ord_month\",\"ord_year\",\"substring(ord_date,9,2) as ord_day\") \\\n.select(\"ord_year\",\"ord_month\",\"ord_date\") \\\n.show(5)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:10:58.835026Z","iopub.execute_input":"2023-06-19T06:10:58.835519Z","iopub.status.idle":"2023-06-19T06:10:59.471161Z","shell.execute_reply.started":"2023-06-19T06:10:58.835473Z","shell.execute_reply":"2023-06-19T06:10:59.469896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ord.withColumn(\"ord_year\",substring(\"ord_date\",1,4)).show(5)\n\nord.withColumn(\"ord_date\",substring(\"ord_date\",1,4)).show(5)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:10:59.472448Z","iopub.execute_input":"2023-06-19T06:10:59.47285Z","iopub.status.idle":"2023-06-19T06:11:00.311071Z","shell.execute_reply.started":"2023-06-19T06:10:59.472812Z","shell.execute_reply":"2023-06-19T06:11:00.309842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ord.withColumnRenamed(\"ord_status\",\"ord_sts\").show(5)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:11:00.312358Z","iopub.execute_input":"2023-06-19T06:11:00.315579Z","iopub.status.idle":"2023-06-19T06:11:00.573162Z","shell.execute_reply.started":"2023-06-19T06:11:00.315528Z","shell.execute_reply":"2023-06-19T06:11:00.57191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ord.drop('ord_status','ord_date').show(2)\nord.drop('ord_status').show(2)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:11:00.574788Z","iopub.execute_input":"2023-06-19T06:11:00.575508Z","iopub.status.idle":"2023-06-19T06:11:01.008802Z","shell.execute_reply.started":"2023-06-19T06:11:00.575344Z","shell.execute_reply":"2023-06-19T06:11:01.007513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emp.show()\n\nemp.dropDuplicates().show() ## only 1 record filtered\n\nemp.dropDuplicates([\"name\"]).show()\n\nemp.dropDuplicates([\"name\",\"score2\"]).show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:11:01.010107Z","iopub.execute_input":"2023-06-19T06:11:01.010636Z","iopub.status.idle":"2023-06-19T06:11:06.360363Z","shell.execute_reply.started":"2023-06-19T06:11:01.010579Z","shell.execute_reply":"2023-06-19T06:11:06.359179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataFrame API Filter\n\n### filter(condition): (Its alias ‘where’)\nFilter rows using a given condition\n\nuse '&' for 'and‘. '|' for 'or‘. (boolean expressions)\n\nUse column function isin() for multiple search Or use IN Operator for SQL Style syntax.","metadata":{}},{"cell_type":"code","source":"# &\nord.where((ord.ordid > 10) & (ord.ordid < 20)).show(5)\n\nord.filter((ord.ordid > 10) & (ord.ordid < 20)).show(5)\n\n# |\nord.where((ord.ordid == 10) | (ord.ordid == 20)).show(5)\n\nord.where((ord.ordid == 10) | (ord.ordid == 20)).show(5)\n\n# isin\nord.where(ord.ord_status.isin(\"CLOSED\",\"PENDING\")).show(5)\nord.filter(ord.ord_status.isin(\"CLOSED\",\"PENDING\")).show(5)\n\n# sql like where optional\nord.where(\"ord_status in ('CLOSED','NOT-PENDING') \").show(5)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:11:06.361651Z","iopub.execute_input":"2023-06-19T06:11:06.36208Z","iopub.status.idle":"2023-06-19T06:11:08.425857Z","shell.execute_reply.started":"2023-06-19T06:11:06.36204Z","shell.execute_reply":"2023-06-19T06:11:08.424592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataFrame APIs Sort\n\n\n### sort() or orderBy()\norderBy() is an alias for .sort()\nSort specific column(s).\n\n### sortWithinPartitions\nAt time, we may not want sort globally, but with in a group. In that case we can use sortWithinPartitions.","metadata":{}},{"cell_type":"code","source":"ord.sort(ord.ord_status.desc()).show(2)\nord.sort(col(\"ord_status\").desc()).show(2)\nord.sort(\"ord_status\",\"ord_cust_id\").show(2)\nord.sort(\"ord_status\",\"ord_cust_id\",ascending=[0,1]).show(2)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:11:08.427748Z","iopub.execute_input":"2023-06-19T06:11:08.428568Z","iopub.status.idle":"2023-06-19T06:11:11.048162Z","shell.execute_reply.started":"2023-06-19T06:11:08.428516Z","shell.execute_reply":"2023-06-19T06:11:11.047027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = [(\"a\",1),(\"b\",2),(\"c\",3),(\"d\",4),(\"e\",5)]\ndf = spark.createDataFrame(data,schema='col1 string,col2 int')\ndf = df.repartition(2)\ndf = df.withColumn(\"partid\",spark_partition_id())\n\ndf.sort(df.col1).show()\ndf.sortWithinPartitions(df.col1).show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:11:11.053754Z","iopub.execute_input":"2023-06-19T06:11:11.054522Z","iopub.status.idle":"2023-06-19T06:11:12.429646Z","shell.execute_reply.started":"2023-06-19T06:11:11.054474Z","shell.execute_reply":"2023-06-19T06:11:12.428379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DataFrame APIs : Set Operators\n\n### union() and unionAll():\nSame and contains duplicate values\nUse distinct after union or unionAll to remove duplicates\n\n### unionByName():\nThe difference between this function and :func:`union` is that this function\nresolves columns by name (not by position)\n\n### intersect(): \nContaining rows in both DataFrames. Removed duplicates\n\n### intersectAll():\nSame as intersect. But retains the duplicates\n\n### exceptAll():\nRows present in one DataFrame but not in another","metadata":{}},{"cell_type":"code","source":"df1 = spark.range(4)\ndf2 = spark.range(3,5)\n\n# Union and UnionAll performs same way\n\ndf1.union(df2).show()\ndf1.unionAll(df2).show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:11:12.431013Z","iopub.execute_input":"2023-06-19T06:11:12.431514Z","iopub.status.idle":"2023-06-19T06:11:12.935105Z","shell.execute_reply.started":"2023-06-19T06:11:12.431464Z","shell.execute_reply":"2023-06-19T06:11:12.933826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.intersect(df2).show()\ndf1.intersectAll(df2).show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:11:12.937089Z","iopub.execute_input":"2023-06-19T06:11:12.937668Z","iopub.status.idle":"2023-06-19T06:11:14.013327Z","shell.execute_reply.started":"2023-06-19T06:11:12.937613Z","shell.execute_reply":"2023-06-19T06:11:14.011833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataFrame APIs : Join\n\nJoin Type\ninner INNER JOIN, JOIN\nouter, full, fullouter,\nfull_outer\nFULL OUTER JOIN\nleft,\nleft_outer,leftouter\nLEFT JOIN\nright,right_outer,rig\nhtouter\nRIGHT JOIN\ncross CROSS JOIN\nleft_anti, leftanti\nleftsemi,left_semi","metadata":{}},{"cell_type":"code","source":"df1 = spark.createDataFrame(data=[(1,'Robert'),(2,'Ria'),(3,'James')],schema='empid int, empname string')\ndf2 = spark.createDataFrame(data=[(2,'USA'),(4,'India')],schema='empid int, country string')\n\n# Inner Join\ndf1.join(df2,df1.empid == df2.empid).select(df1.empid.alias(\"employeeid\"),df1.empname,df2.country).show()\n\n# left anti join\n# cannot get right table attributes and it executes faster as compared to inner as it doesn't check right/second table attributes\ndf1.join(df2,df1.empid == df2.empid,\"leftanti\").show()\n\n# Left outer or Left\ndf1.join(df2,df1.empid == df2.empid,\"left\")\\\n.select(df1.empid,df1.empname,df2.country) \\\n.sort(df2.country.desc()) \\\n.show()\n\n# full outer\ndf1.join(df2,df1.empid == df2.empid,\"full\")\\\n.select(df1.empid,df1.empname,df2.country) \\\n.sort(df1.empid.asc()) \\\n.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:11:14.014741Z","iopub.execute_input":"2023-06-19T06:11:14.015457Z","iopub.status.idle":"2023-06-19T06:11:19.509467Z","shell.execute_reply.started":"2023-06-19T06:11:14.015369Z","shell.execute_reply":"2023-06-19T06:11:19.508328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# self\n\ndf1 = spark.createDataFrame(data=((1,'Robert',2),(2,'Ria',3),(3,'James',5)),schema='empid int,empname string,managerid int')\ndf1.show()\ndf1.alias(\"emp1\").join(df1.alias(\"emp2\"),col(\"emp1.managerid\") == col(\"emp2.empid\"))\\\n.select(col(\"emp1.empid\").alias(\"EmployeeID\"),col(\"emp1.empname\").alias(\"EmployeeName\"),col(\"emp2.empid\").alias(\"ManagerID\"),col(\"emp2.empname\").alias(\"ManagerName\")) \\\n.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:11:19.51074Z","iopub.execute_input":"2023-06-19T06:11:19.511161Z","iopub.status.idle":"2023-06-19T06:11:21.301479Z","shell.execute_reply.started":"2023-06-19T06:11:19.511121Z","shell.execute_reply":"2023-06-19T06:11:21.300114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Window Functions\n\nLets see 3 types of Window Functions:\nRanking\nAnalytical\nAggregate","metadata":{"execution":{"iopub.status.busy":"2023-06-16T11:38:46.67084Z","iopub.execute_input":"2023-06-16T11:38:46.671857Z","iopub.status.idle":"2023-06-16T11:38:46.676326Z","shell.execute_reply.started":"2023-06-16T11:38:46.671816Z","shell.execute_reply":"2023-06-16T11:38:46.675271Z"}}},{"cell_type":"code","source":"data = ((\"James\",\"Sales\",\"NY\",9000,34),\n(\"Alicia\",\"Sales\",\"NY\",8600,56),\n(\"Robert\",\"Sales\",\"CA\",8100,30),\n(\"John\",\"Sales\",\"AZ\",8600,31),\n(\"Ross\",\"Sales\",\"AZ\",8100,33),\n(\"Kathy\",\"Sales\",\"AZ\",1000,39),\n(\"Lisa\",\"Finance\",\"CA\",9000,24),\n(\"Deja\",\"Finance\",\"CA\",9900,40),\n(\"Sugie\",\"Finance\",\"NY\",8300,36),\n(\"Ram\",\"Finance\",\"NY\",7900,53),\n(\"Satya\",\"Finance\",\"AZ\",8200,53),\n(\"Kyle\",\"Marketing\",\"CA\",8000,25),\n(\"Reid\",\"Marketing\",\"NY\",9100,50)\n)\n\nschema=(\"empname\",\"dept\",\"state\",\"salary\",\"age\")\n\ndf = spark.createDataFrame(data=data,schema=schema)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:11:21.302895Z","iopub.execute_input":"2023-06-19T06:11:21.303381Z","iopub.status.idle":"2023-06-19T06:11:21.344601Z","shell.execute_reply.started":"2023-06-19T06:11:21.303337Z","shell.execute_reply":"2023-06-19T06:11:21.343729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spec = Window.partitionBy(\"dept\").orderBy(col(\"salary\").desc())\n\ndf.withColumn(\"rank\",rank().over(spec))\\\n.withColumn(\"drank\",dense_rank().over(spec))\\\n.withColumn(\"rownum\",row_number().over(spec))\\\n.withColumn(\"perrnk\",percent_rank().over(spec))\\\n.withColumn(\"perrnk\",percent_rank().over(spec))\\\n.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:11:21.347273Z","iopub.execute_input":"2023-06-19T06:11:21.348466Z","iopub.status.idle":"2023-06-19T06:11:22.325106Z","shell.execute_reply.started":"2023-06-19T06:11:21.348391Z","shell.execute_reply":"2023-06-19T06:11:22.323704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spec = Window.partitionBy(\"dept\").orderBy(col(\"salary\").desc())\n\ndf.select(\"dept\",\"salary\")\\\n.withColumn(\"lead_salary\",lead(\"salary\",1,-1).over(spec))\\\n.withColumn(\"lag_salary\",lag(\"salary\",1,-1).over(spec))\\\n.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:11:22.326471Z","iopub.execute_input":"2023-06-19T06:11:22.326916Z","iopub.status.idle":"2023-06-19T06:11:23.121724Z","shell.execute_reply.started":"2023-06-19T06:11:22.326877Z","shell.execute_reply":"2023-06-19T06:11:23.120513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spec_without_orderby = Window.partitionBy(\"dept\")\nspec_with_orderby = Window.partitionBy(\"dept\").orderBy(col(\"salary\").asc()) ## rolling sum if we use order by\n\ndf.select(\"dept\",\"salary\")\\\n.withColumn(\"sum_slry_without_orderby\",sum(\"salary\").over(spec_without_orderby))\\\n.withColumn(\"sum_slry_with_orderby\",sum(\"salary\").over(spec_with_orderby))\\\n.withColumn(\"avg_slry_without_orderby\",avg(\"salary\").over(spec_without_orderby))\\\n.withColumn(\"avg_slry_with_orderby\",avg(\"salary\").over(spec_with_orderby))\\\n.withColumn(\"min_salary\",min(\"salary\").over(spec_without_orderby))\\\n.withColumn(\"max_salary\",max(\"salary\").over(spec_without_orderby))\\\n.withColumn(\"count_salary\",count(\"salary\").over(spec_without_orderby))\\\n.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:11:23.123122Z","iopub.execute_input":"2023-06-19T06:11:23.125784Z","iopub.status.idle":"2023-06-19T06:11:24.35316Z","shell.execute_reply.started":"2023-06-19T06:11:23.125739Z","shell.execute_reply":"2023-06-19T06:11:24.35197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Window Functions\n\n* rangeBetween:\n    - Takes two argument (start,end) to define frame boundaries.\n    - Default : unboundedPreceding and unboundedFollowing\n\n* rowsBetween:\n    - Takes two argument (start,end) to define frame boundaries.\n    - Deafult : unboundedPreceding and unboundedFollowing.","metadata":{}},{"cell_type":"code","source":"spec1=Window.partitionBy(df.dept).orderBy(df.salary).rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\ndf.select(df.dept,df.salary).withColumn(\"sum_sal\",sum(\"salary\").over(spec1)).show()\nspec1=Window.partitionBy(df.dept).orderBy(df.salary).rangeBetween(Window.currentRow, Window.unboundedFollowing)\ndf.select(df.dept,df.salary).withColumn(\"sum_sal\",sum(\"salary\").over(spec1)).show()\nspec1=Window.partitionBy(df.dept).orderBy(df.salary).rangeBetween(Window.currentRow,500)\ndf.select(df.dept,df.salary).withColumn(\"sum_sal\",sum(\"salary\").over(spec1)).show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:12:05.795777Z","iopub.execute_input":"2023-06-19T06:12:05.796235Z","iopub.status.idle":"2023-06-19T06:12:07.585933Z","shell.execute_reply.started":"2023-06-19T06:12:05.7962Z","shell.execute_reply":"2023-06-19T06:12:07.584807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataFrame APIs : Aggregations\n\n    - Summary\n    - avg, min, max, sum, sumDistinct, count, countDistinct -- all can be used in same/single `select`\n    - df = spark.createDataFrame(((1,100),(2,150),(3,200),(4,50),(5,50)),schema='id int,salary int')\n    - df.select(collect_list(df.salary)).show(truncate=False)\n","metadata":{}},{"cell_type":"code","source":"ord_items = \"/kaggle/input/retail-db-tutorial/RetailDB SalesData/Order_items/part-00000\"\n\norditems = spark.read.format(\"csv\").load(ord_items,sep=\",\",schema='order_item_id int,ord_item_order_id int, order_item_product_id int, quantity int,subtotal float, price float')\norditems.summary().show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:16:17.381446Z","iopub.execute_input":"2023-06-19T06:16:17.381914Z","iopub.status.idle":"2023-06-19T06:16:20.643944Z","shell.execute_reply.started":"2023-06-19T06:16:17.381879Z","shell.execute_reply":"2023-06-19T06:16:20.642743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orditems.select(\"price\").summary().show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:16:41.742175Z","iopub.execute_input":"2023-06-19T06:16:41.742595Z","iopub.status.idle":"2023-06-19T06:16:42.888116Z","shell.execute_reply.started":"2023-06-19T06:16:41.742561Z","shell.execute_reply":"2023-06-19T06:16:42.886562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orditems.select(avg(\"price\").alias(\"avg_price\"),min(\"price\").alias(\"min_price\"),max(\"price\").alias(\"max_price\")).show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:20:38.900422Z","iopub.execute_input":"2023-06-19T06:20:38.900831Z","iopub.status.idle":"2023-06-19T06:20:39.273093Z","shell.execute_reply.started":"2023-06-19T06:20:38.900802Z","shell.execute_reply":"2023-06-19T06:20:39.271854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orditems.select(avg(\"price\").alias(\"avg_price\"),\n                min(\"price\").alias(\"min_price\"),\n                max(\"price\").alias(\"max_price\"),\n                sum(\"price\").alias(\"sum_price\"),\n                sumDistinct(\"price\").alias(\"sum_distinct_price\")\n               ).show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:22:18.135385Z","iopub.execute_input":"2023-06-19T06:22:18.135804Z","iopub.status.idle":"2023-06-19T06:22:18.753339Z","shell.execute_reply.started":"2023-06-19T06:22:18.135773Z","shell.execute_reply":"2023-06-19T06:22:18.752216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orditems.select(avg(\"price\").alias(\"avg_price\"),\n                min(\"price\").alias(\"min_price\"),\n                max(\"price\").alias(\"max_price\"),\n                sum(\"price\").alias(\"sum_price\"),\n                sumDistinct(\"price\").alias(\"sum_distinct_price\"),\n                count(\"price\").alias(\"count_price\"),\n                countDistinct(\"price\").alias(\"distinct_price_count\")\n               ).show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:24:08.029144Z","iopub.execute_input":"2023-06-19T06:24:08.029585Z","iopub.status.idle":"2023-06-19T06:24:08.56232Z","shell.execute_reply.started":"2023-06-19T06:24:08.029551Z","shell.execute_reply":"2023-06-19T06:24:08.56138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = spark.createDataFrame(((1,100),(2,150),(3,200),(4,50),(5,50)),schema='id int,salary int')\ndf.show()\ndf.select(collect_list(df.salary)).show(truncate=False) ## shows duplicate value as well\ndf.select(collect_set(df.salary)).show(truncate=False)  ## Only shows unique values","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:30:08.787098Z","iopub.execute_input":"2023-06-19T06:30:08.787549Z","iopub.status.idle":"2023-06-19T06:30:10.852846Z","shell.execute_reply.started":"2023-06-19T06:30:08.787513Z","shell.execute_reply":"2023-06-19T06:30:10.851248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataFrame API GroupBy","metadata":{}},{"cell_type":"code","source":"data = ((\"James\",\"Sales\",\"NY\",9000,34),\n(\"Alicia\",\"Sales\",\"NY\",8600,56),\n(\"Robert\",\"Sales\",\"CA\",8100,30),\n(\"Lisa\",\"Finance\",\"CA\",9000,24),\n(\"Deja\",\"Finance\",\"CA\",9900,40),\n(\"Sugie\",\"Finance\",\"NY\",8300,36),\n(\"Ram\",\"Finance\",\"NY\",7900,53),\n(\"Kyle\",\"Marketing\",\"CA\",8000,25),\n(\"Reid\",\"Marketing\",\"NY\",9100,50))\n\nschema=(\"empname\",\"dept\",\"state\",\"salary\",\"age\")\ndf = spark.createDataFrame(data=data,schema=schema)\ndf.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:39:28.518237Z","iopub.execute_input":"2023-06-19T06:39:28.518626Z","iopub.status.idle":"2023-06-19T06:39:29.176905Z","shell.execute_reply.started":"2023-06-19T06:39:28.518595Z","shell.execute_reply":"2023-06-19T06:39:29.175661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filtering records before performing aggregations","metadata":{}},{"cell_type":"code","source":"groupped_data = df.where(df.state == \"NY\").groupBy(\"dept\")\ngroupped_data.agg(min(\"salary\").alias(\"min_salary\"),max(\"salary\").alias(\"max_salary\"),avg(\"salary\").alias(\"avg_salary\")).show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:57:26.947744Z","iopub.execute_input":"2023-06-19T06:57:26.948172Z","iopub.status.idle":"2023-06-19T06:57:27.857369Z","shell.execute_reply.started":"2023-06-19T06:57:26.94814Z","shell.execute_reply":"2023-06-19T06:57:27.856267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filtering records after performing aggregations","metadata":{}},{"cell_type":"code","source":"groupped_data = df.groupBy(\"dept\")\ngroupped_data.agg(min(\"salary\").alias(\"min_salary\"),\n                  max(\"salary\").alias(\"max_salary\"),\n                  avg(\"salary\").alias(\"avg_salary\"))\\\n.where(col(\"min_salary\") >= 8000)\\\n.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:58:15.463068Z","iopub.execute_input":"2023-06-19T06:58:15.463898Z","iopub.status.idle":"2023-06-19T06:58:16.220748Z","shell.execute_reply.started":"2023-06-19T06:58:15.463853Z","shell.execute_reply":"2023-06-19T06:58:16.219544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Without Filters","metadata":{}},{"cell_type":"code","source":"groupped_data = df.groupBy(\"dept\")\ngroupped_data.agg(min(\"salary\").alias(\"min_salary\"),\n                  max(\"salary\").alias(\"max_salary\"),\n                  avg(\"salary\").alias(\"avg_salary\"))\\\n.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T06:58:43.075068Z","iopub.execute_input":"2023-06-19T06:58:43.075461Z","iopub.status.idle":"2023-06-19T06:58:43.646974Z","shell.execute_reply.started":"2023-06-19T06:58:43.075431Z","shell.execute_reply":"2023-06-19T06:58:43.645825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataFrame APIs : Null Functions","metadata":{}},{"cell_type":"code","source":"# Prepare Data\ndf = spark.createDataFrame((('Robert',1, None,114.0), ('John',None, 2577,float('nan'))), (\"name\", \"id\",\"phone\",\"stAdd\"))\ndf.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T07:15:54.668096Z","iopub.execute_input":"2023-06-19T07:15:54.668908Z","iopub.status.idle":"2023-06-19T07:15:55.360595Z","shell.execute_reply.started":"2023-06-19T07:15:54.668872Z","shell.execute_reply":"2023-06-19T07:15:55.359185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To check Null values\ndf.select(\"phone\",isnull(\"phone\").alias(\"nullcheck\")).show()\n\n## Advance filtering based on true false\ndf.select(\"phone\",isnull(\"phone\").alias(\"nullcheck\")).filter(col(\"nullcheck\")).show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T07:17:17.173724Z","iopub.execute_input":"2023-06-19T07:17:17.174157Z","iopub.status.idle":"2023-06-19T07:17:18.231716Z","shell.execute_reply.started":"2023-06-19T07:17:17.174124Z","shell.execute_reply":"2023-06-19T07:17:18.230473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To check NaN == Not a Number values\ndf.select(\"stAdd\",isnan(\"stAdd\").alias(\"nullcheck\")).show()\n\n## Advance filtering based on true false\ndf.select(\"stAdd\",isnan(\"stAdd\").alias(\"nullcheck\")).filter(col(\"nullcheck\")).show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T07:18:15.610115Z","iopub.execute_input":"2023-06-19T07:18:15.610571Z","iopub.status.idle":"2023-06-19T07:18:16.708893Z","shell.execute_reply.started":"2023-06-19T07:18:15.610539Z","shell.execute_reply":"2023-06-19T07:18:16.707689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Returns col1 if it is not NaN, or col2 if col1 is NaN.\ndf.select(df.stAdd,df.phone,nanvl(df.stAdd,df.phone)).show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T07:18:37.043116Z","iopub.execute_input":"2023-06-19T07:18:37.043494Z","iopub.status.idle":"2023-06-19T07:18:37.67597Z","shell.execute_reply.started":"2023-06-19T07:18:37.043465Z","shell.execute_reply":"2023-06-19T07:18:37.674806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Very Important\n\n##### Drop can drop any rows and columns","metadata":{}},{"cell_type":"code","source":"## Drop can drop columns and rows\ndata=(('Alice',80,10),('Bob',None,5),('Tom',50,50),(None,None,None),('Robert',30,35))\nschema='name string, age int, height int'\ndf = spark.createDataFrame(data,schema)\ndf.show()\ndf.na.drop().show()\ndf.where(~isnull(\"age\")).drop().show()\ndf.drop(df.age).show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T07:22:53.999321Z","iopub.execute_input":"2023-06-19T07:22:53.999741Z","iopub.status.idle":"2023-06-19T07:22:56.311197Z","shell.execute_reply.started":"2023-06-19T07:22:53.999708Z","shell.execute_reply":"2023-06-19T07:22:56.310013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.na.fill(-1).show()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T07:24:26.464204Z","iopub.execute_input":"2023-06-19T07:24:26.464601Z","iopub.status.idle":"2023-06-19T07:24:27.014731Z","shell.execute_reply.started":"2023-06-19T07:24:26.464571Z","shell.execute_reply":"2023-06-19T07:24:27.013867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### emp2 DataFrame\ndata=(('Robert',35,40,40),('Ram',31,33,29),('John',95,89,91))\nschema = ('name','score1','score2','score3')\nemp2= spark.createDataFrame(data=data, schema=schema)\n\nemp2.select(emp2.score1, emp2.score2,sequence(emp2.score1,emp2.score2).alias('new_col')).show(truncate=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T07:14:44.992118Z","iopub.execute_input":"2023-06-19T07:14:44.992711Z","iopub.status.idle":"2023-06-19T07:14:45.691973Z","shell.execute_reply.started":"2023-06-19T07:14:44.99267Z","shell.execute_reply":"2023-06-19T07:14:45.690813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read & Write Files\n\n\n    - CSV\n    - JSON\n    - Parquet\n    - XML, Avro, Binary","metadata":{}},{"cell_type":"markdown","source":"## Reading CSV\n\n    - Read multiple CSV files\n    - Read all CSV files in a directory\n            ``spark.read.csv(\"path\") & dataframe.write.csv(\"path\")``\n    - Options while reading CSV file\n            * delimiter\n            * InferSchema\n            * header\n            * quotes\n            * nullValues\n            * dateFormat\n    - Write DataFrame to CSV file\n            * Using options\n            * Saving Mode","metadata":{}},{"cell_type":"code","source":"spark.read.csv(path1).option(\"header\",True)\nspark.read.csv(\"path1,path2,path3\").option(\"header\",True).option()\nspark.read.csv(Folderpath).option(\"header\",True).option()","metadata":{},"execution_count":null,"outputs":[]}]}